太棒了！这是一个非常有深度和挑战性的项目，堪称“PyTorch匠人”的试炼。从最底层的张量操作出发，一直到高性能的自定义CUDA Kernel，这完美地展示了PyTorch从灵活性到极致性能的全方位能力。

我们来为这个激动人心的项目开个好头。我们将分阶段进行，第一阶段是打好坚实的基础：**用纯张量操作构建一个功能完整的、采用Pre-LN架构的Transformer Encoder**。

这第一步将为你后续的性能分析、CUDA Kernel替换和高级优化提供一个坚实的基准（Baseline）。

---

### 项目启动：匠人级Transformer (Artisan-Transformer)

#### 阶段一：纯张量操作的基石

**目标**：不依赖 `nn.MultiheadAttention` 或 `nn.TransformerEncoderLayer`，手动实现所有核心组件。

**项目结构建议**:

```
artisan_transformer/
├── main.py             # 主执行文件，用于测试和训练
├── model.py            # Transformer模型定义
├── modules.py          # 核心组件，如手动实现的Multi-Head Attention, FFN等
└── utils.py            # 工具函数，如mask的创建
```

---

#### 1. `utils.py`: 创建Mask的艺术

首先，我们来实现两个至关重要的masking函数。

```python
# artisan_transformer/utils.py

import torch

def create_padding_mask(seq_k, pad_idx=0):
    """
    为输入的序列创建padding mask。
    Args:
        seq_k (torch.Tensor): 输入序列，形状为 (B, S_k)。通常是token IDs。
        pad_idx (int): padding token的ID。
    Returns:
        torch.Tensor: Padding mask，形状为 (B, 1, 1, S_k)，方便在Attention中进行广播。
                      值为True的部分是需要被mask掉的。
    """
    # (B, S_k) -> (B, 1, 1, S_k)
    return (seq_k == pad_idx).unsqueeze(1).unsqueeze(2)

def create_look_ahead_mask(seq_len):
    """
    为Decoder创建look-ahead mask (或称causal mask)。
    Args:
        seq_len (int): 序列的长度。
    Returns:
        torch.Tensor: Look-ahead mask，形状为 (1, 1, S, S)。
                      上三角部分为True，表示不能attend to future tokens。
    """
    # (S, S)
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    # (1, 1, S, S) for broadcasting
    return mask.unsqueeze(0).unsqueeze(0)

```
**关键点**：
*   我们精心调整了mask的形状，使其能够与Attention Scores矩阵 `(B, H, S, S)` 进行广播（broadcasting）操作，这是非常高效的做法。

---

#### 2. `modules.py`: 手撕核心组件

这是项目的核心。我们将在这里用基础张量操作实现多头注意力。这里我们使用`einops`来展示其简洁性，同时也会用注释标明等价的`reshape/transpose`操作。

```python
# artisan_transformer/modules.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from einops import rearrange, repeat

class HandcraftedMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_head = d_model // num_heads

        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)

    def forward(self, q, k, v, mask=None):
        # 1. 线性投影
        # q, k, v: (B, S, D_model)
        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)

        # 2. 拆分多头 (The magic happens here!)
        # (B, S, D_model) -> (B, H, S, D_head)
        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)
        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)
        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)
        # -- 等价的 reshape/transpose 操作 --
        # B, S, D = q.shape
        # q = q.view(B, S, self.num_heads, self.d_head).transpose(1, 2) # (B, H, S, D_head)
        # k = k.view(B, S, self.num_heads, self.d_head).transpose(1, 2)
        # v = v.view(B, S, self.num_heads, self.d_head).transpose(1, 2)

        # 3. 计算Scaled Dot-Product Attention
        # (B, H, S_q, D_head) @ (B, H, D_head, S_k) -> (B, H, S_q, S_k)
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)
        
        # 4. 应用Mask (非常重要！)
        if mask is not None:
            # mask的形状会被广播到 (B, H, S_q, S_k)
            attn_scores = attn_scores.masked_fill(mask, -1e9)

        # 5. Softmax
        attn_probs = F.softmax(attn_scores, dim=-1)
        # 这里可以加一个dropout
        # attn_probs = F.dropout(attn_probs, p=self.dropout_p)

        # 6. 与V相乘得到输出
        # (B, H, S_q, S_k) @ (B, H, S_v, D_head) -> (B, H, S_q, D_head)
        # 注意 S_k == S_v
        output = torch.matmul(attn_probs, v)

        # 7. 合并多头
        # (B, H, S, D_head) -> (B, S, D_model)
        output = rearrange(output, 'b h s d -> b s (h d)')
        # -- 等价的 reshape/transpose 操作 --
        # output = output.transpose(1, 2).contiguous().view(B, S, D)

        # 8. 最后线性投影
        return self.w_o(output)


class PositionwiseFeedForward(nn.Module):
    """这个模块是后续自定义CUDA Kernel的绝佳目标"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.activation = nn.GELU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, S, D_model)
        return self.linear2(self.dropout(self.activation(self.linear1(x))))

```
**关键点**：
*   我们完全手动地实现了QKV的投影、多头拆分、注意力计算、mask应用、多头合并和最后输出的全过程。
*   `einops` 极大地简化了张量变形的代码，使其更具可读性。
*   `PositionwiseFeedForward` 看起来简单，但 `Linear -> GELU -> Dropout -> Linear` 的组合操作在实践中非常耗时，是融合（Fuse）优化的理想候选。

---

#### 3. `model.py`: 组装 Pre-LN Transformer Encoder

现在我们将上述组件组装成一个完整的Encoder Layer，并采用 **Pre-LN** 架构。Pre-LN（先LayerNorm再进行子层计算）比Post-LN在训练时更稳定，是现代大型模型（如GPT系列）的标配。

```python
# artisan_transformer/model.py

import torch
import torch.nn as nn
from .modules import HandcraftedMultiHeadAttention, PositionwiseFeedForward

class PreLNEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = HandcraftedMultiHeadAttention(d_model, num_heads)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        
        # Pre-LN 架构的关键：两个LayerNorm层分别位于子模块之前
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # x: (B, S, D_model)
        
        # 1. Multi-Head Attention (with Pre-LN)
        residual = x
        x_norm = self.norm1(x)
        attn_output = self.self_attn(x_norm, x_norm, x_norm, mask)
        x = residual + self.dropout1(attn_output)
        
        # 2. Feed Forward (with Pre-LN)
        residual = x
        x_norm = self.norm2(x)
        ffn_output = self.ffn(x_norm)
        x = residual + self.dropout2(ffn_output)
        
        return x

class ArtisanTransformerEncoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))
        self.layers = nn.ModuleList([
            PreLNEncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src_tokens, padding_mask=None):
        # src_tokens: (B, S)
        B, S = src_tokens.shape
        x = self.embedding(src_tokens) + self.pos_encoding[:, :S, :]
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, padding_mask)
            
        return x
```

---

#### 4. `main.py`: 启动与验证

最后，创建一个主文件来实例化并测试我们的模型，确保一切正常工作。

```python
# artisan_transformer/main.py

import torch
from artisan_transformer.model import ArtisanTransformerEncoder
from artisan_transformer.utils import create_padding_mask

def test_artisan_transformer():
    # --- 模型参数 ---
    vocab_size = 1000
    num_layers = 4
    d_model = 256
    num_heads = 8
    d_ff = 1024
    max_seq_len = 100
    batch_size = 16
    seq_len = 50
    pad_idx = 0
    
    # --- 设备 ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # --- 实例化模型 ---
    model = ArtisanTransformerEncoder(
        num_layers=num_layers,
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        vocab_size=vocab_size,
        max_seq_len=max_seq_len
    ).to(device)

    print(f"Model created. Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # --- 创建伪数据 ---
    # (B, S), 其中一些序列有padding
    src_tokens = torch.randint(1, vocab_size, (batch_size, seq_len), device=device)
    src_tokens[0, seq_len-10:] = pad_idx # 给第一个样本加上padding
    src_tokens[1, seq_len-5:] = pad_idx  # 给第二个样本加上padding

    # --- 创建Padding Mask ---
    padding_mask = create_padding_mask(src_tokens, pad_idx).to(device)
    
    print("\n--- Running a forward pass ---")
    # --- 前向传播 ---
    output = model(src_tokens, padding_mask)

    print(f"Input shape: {src_tokens.shape}")
    print(f"Padding mask shape: {padding_mask.shape}")
    print(f"Output shape: {output.shape}")

    # --- 验证输出 ---
    assert output.shape == (batch_size, seq_len, d_model)
    print("\n✅ Forward pass successful! The shapes are correct.")
    print("Your Artisan Transformer's foundation is solid.")


if __name__ == "__main__":
    test_artisan_transformer()

```

### 下一步，你的王牌 (Next Steps: Your Ace)

你现在拥有了一个坚实、纯净、且高度可定制的Transformer基座。接下来，你可以按计划推进：

1.  **性能分析 (Pillar 3 - Part 1)**:
    *   在`main.py`中，使用 `torch.profiler` 包裹`model(src_tokens, padding_mask)`的调用。
    *   运行profiler并查看结果，找出耗时最长的操作。你会发现`HandcraftedMultiHeadAttention`和`PositionwiseFeedForward`是两大热点。
    *   使用`torch.cuda.memory_summary()`来检查显存占用情况。

2.  **自定义CUDA Kernel (Pillar 2)**:
    *   **选择目标**: `PositionwiseFeedForward` 是完美的起点。你可以写一个Triton Kernel来融合`Linear -> GELU -> Dropout`。
    *   **实现方式**:
        *   **Triton (推荐)**: 在Python中直接编写一个`@triton.jit`装饰的函数，它会自动编译成CUDA Kernel。这对于快速迭代和与PyTorch的集成是无与伦比的。
        *   **C++ Extension**: 编写`.cpp`和`.cu`文件，使用`torch::Tensor`，然后通过`setup.py`和`torch.utils.cpp_extension.load()`来加载。这更底层，控制力更强，但开发周期更长。
    *   **集成**: 在`PositionwiseFeedForward`模块中，用`if self.training and use_custom_kernel:`这样的逻辑来调用你的自定义算子，并与原生PyTorch实现进行性能和精度对比。

3.  **精细优化 (Pillar 3 - Part 2)**:
    *   **梯度检查点 (Gradient Checkpointing)**: 在`ArtisanTransformerEncoder`的`forward`循环中，用`torch.utils.checkpoint.checkpoint`包裹`layer(x, padding_mask)`的调用。这将显著降低显存占用，让你能训练更大的模型。
    *   **对比Pre-LN vs Post-LN**: 实现一个`PostLNEncoderLayer`，并尝试用它来训练模型。观察在训练初期loss是否容易出现`NaN`，以及收敛速度和最终性能与Pre-LN的差异。

这个开头为你铺平了道路。你已经掌握了最核心的张量操作，现在可以开始真正的“炫技”了。祝你在这个项目中玩得开心，并打造出一个真正属于你自己的、高性能的Transformer！